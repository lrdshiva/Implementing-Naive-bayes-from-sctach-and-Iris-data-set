{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive_Bayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/lrdshiva/Implementing-Naive-bayes-from-sctach-and-Iris-data-set/blob/master/Naive_Bayes.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "esieqr4omXu7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "<a href=\"https://ibb.co/g0Q5PK\"><img src=\"https://preview.ibb.co/he0uJe/explanation_1.jpg\" alt=\"explanation_1\" border=\"0\"></a>\n",
        "<a href=\"https://ibb.co/bVGJde\"><img src=\"https://preview.ibb.co/j5W6Wz/explanation_2.jpg\" alt=\"explanation_2\" border=\"0\"></a>\n",
        "<a href=\"https://ibb.co/fj2kPK\"><img src=\"https://preview.ibb.co/mQGqrz/explanation_3.jpg\" alt=\"explanation_3\" border=\"0\"></a>\n",
        "\n",
        "##**Below is the class that is a direct implementation on the classifier alogn with example of a synthetic data and iris data.**##"
      ]
    },
    {
      "metadata": {
        "id": "1Xa8z37fp1As",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class naive_bayes:\n",
        "  \n",
        "  def __init__(self,method = 'Gausian'):\n",
        "    \n",
        "    self.classes = {} #dictionary to sort the data class wise.\n",
        "    self.mean = {}\n",
        "    self.varience = {}\n",
        "    self.P_classes = {} # to store prob of the vector belonging to a certain class Ck\n",
        "    self.pi = 3.14159\n",
        "    self.P_cl = {} # prob. of occurance of a certain class or prior\n",
        "    \n",
        "  def train(self,X,y):\n",
        "    \n",
        "    y = y.reshape(-1,1)\n",
        "    \n",
        "    self.gaussian(X,y)\n",
        "  \n",
        "  def predict(self, X):\n",
        "    \n",
        "    y = np.ones((X.shape[0],1))\n",
        "    \n",
        "    # take each example\n",
        "    for n in range(X.shape[0]):\n",
        "      \n",
        "      x=X[n,:].reshape(1,-1)\n",
        "      max_p = 0\n",
        "      sum_p = 0\n",
        "      \n",
        "      \n",
        "      #select a class and find the coresponding prob\n",
        "      for i in range(self.unique.shape[0]):\n",
        "        \n",
        "        sum_p = 0\n",
        "        self.P_cl[\"class\"+str(i)] =1\n",
        "        pcl =   self.P_classes[\"class\"+str(i)]\n",
        "        \n",
        "        #find P given each feature in Xn\n",
        "        for j in range(X.shape[1]):\n",
        "          \n",
        "          mean =  self.mean[\"class\"+str(i)][j]\n",
        "          var =   self.varience[\"class\"+str(i)][j]\n",
        "          \n",
        "          #print(self.P_gaussian(x[0,j],mean,var))\n",
        "          #likelihood\n",
        "          self.P_cl[\"class\"+str(i)] *= self.P_gaussian(x[0,j],mean,var)\n",
        "        \n",
        "        #multiply with prior  \n",
        "        self.P_cl[\"class\"+str(i)] *= pcl\n",
        "        \n",
        "        #evidence\n",
        "        sum_p += self.P_cl[\"class\"+str(i)]\n",
        "        \n",
        "        #print( \"prob = \" , self.P_cl[\"class\"+str(i)] )\n",
        "        \n",
        "        if(self.P_cl[\"class\"+str(i)] > max_p ):\n",
        "          \n",
        "          max_p = self.P_cl[\"class\"+str(i)]\n",
        "          label  = self.unique[i]\n",
        "      #update and store the label which is most probable among them\n",
        "      y[n] = label  \n",
        "    return y\n",
        "  \n",
        "  \n",
        "  # this function finds the unique labels and gathers the data belonging to that class\n",
        "  def gaussian(self,X,y):\n",
        "    \n",
        "    self.unique = np.unique(y)\n",
        "    \n",
        "    for i in range(self.unique.shape[0]):\n",
        "      \n",
        "      index = np.argwhere(y == self.unique[i])\n",
        "      \n",
        "      #print(index,\"  d  \")\n",
        "      \n",
        "      self.classes[\"class\"+str(i)] = X[index[:,0],:]\n",
        "      \n",
        "      self.mean[\"class\"+str(i)] = np.mean(self.classes[\"class\"+str(i)] , axis = 0)\n",
        "      # note that the varience is sample varience.\n",
        "      self.varience[\"class\"+str(i)] = np.sum( np.square(self.classes[\"class\"+str(i)] - self.mean[\"class\"+str(i)]) , axis = 0) / (self.classes[\"class\"+str(i)].shape[0]-1)\n",
        "      # probabilty of occurance of each class from the train data\n",
        "      self.P_classes[\"class\"+str(i)] = self.classes[\"class\"+str(i)].shape[0]/X.shape[0]\n",
        "  \n",
        "  #this function finds the gaussian probability\n",
        "  def P_gaussian(self,x,mean,var):\n",
        "    \n",
        "    A = 1/(((2*self.pi*var)**0.5))\n",
        "    \n",
        "    power = np.square(x-mean)/(2*var)\n",
        "    \n",
        "    P = A * np.exp(-power)\n",
        "    \n",
        "    return P\n",
        "  \n",
        "  def score(self,test,predicted):\n",
        "    \n",
        "    return confusion_matrix(test, predicted)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vMae7qZwkyEh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Examples to test the algorithm: ##"
      ]
    },
    {
      "metadata": {
        "id": "FREqMJOokpKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "95e484bb-6c58-4ebb-bdf0-6f189125d267"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "D = np.array([[1, 6.0 , 180,\t12],\n",
        "              [1,\t5.92,\t190,\t11],\n",
        "              [1,\t5.58, 170,\t12],\n",
        "              [1,\t5.92,\t165,\t10],\n",
        "              [2,\t5.0 ,\t100,\t 6],\n",
        "              [2,\t5.5 ,\t150,\t 8],\n",
        "              [2,\t5.42,\t130,\t 7],\n",
        "              [2,\t5.75,\t150,\t 9]])\n",
        "\n",
        "X = D[:,1:].reshape(8,3)\n",
        "#print(X)\n",
        "y = D[:,0].reshape(-1,1)\n",
        "\n",
        "nb = naive_bayes(method = 'Gausian')\n",
        "nb.train(X,y)\n",
        "print(nb.predict(X))\n",
        "print(nb.predict(np.array([[6,130,8]])))\n",
        "#print(nb.classes)\n",
        "#print(nb.mean)\n",
        "#print(nb.varience)\n",
        "#print(nb.P_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]]\n",
            "[[2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "92fjpOKlkTHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Application of the algorithm on Iris data set##"
      ]
    },
    {
      "metadata": {
        "id": "pNgqps23IH4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdff928f-1d4d-4eeb-c9e6-f9bbc3de0576"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris_data = datasets.load_iris()\n",
        "X = iris_data.data\n",
        "y = iris_data.target.reshape(-1,1)\n",
        "\n",
        "\n",
        "print(X.shape,\"\",y.shape)\n",
        "data =np.concatenate((y,X),axis=1)\n",
        "#data is shuffled as the data set is sorted\n",
        "# we are doing so that training is done properly\n",
        "np.random.shuffle(data)\n",
        "\n",
        "X_train = data[:20,1:].reshape(-1,4)\n",
        "y_train = data[:20,0].reshape(-1,1)\n",
        "\n",
        "X_test = data[21:,1:]\n",
        "y_test = data[21:,0 ].reshape(-1,1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 4)  (150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hWhTFiHXWS5R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "29b411a9-3b91-4624-e7e4-450f251baf15"
      },
      "cell_type": "code",
      "source": [
        "bayes_iris = naive_bayes(method = 'Gausian')\n",
        "bayes_iris.train(X_train,y_train)\n",
        "yp = bayes_iris.predict(X_test)\n",
        "\n",
        "#for i in range(y_test.shape[0]):\n",
        "  \n",
        "  #print(\"pred = \",yp[i], \"  test = \", y_test[i])\n",
        "  \n",
        "  \n",
        "score = bayes_iris.score(y_test,yp)\n",
        "print(\"confusion matrix:\")\n",
        "print(score)\n",
        "print(\"precision = sum of the diagonal element/total number to test data \")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            "[[45  0  0]\n",
            " [ 0 36  5]\n",
            " [ 0  2 41]]\n",
            "precision = sum of the diagonal element/total number to test data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_6kRD2PylO1t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##conclusion##\n",
        "The naive Bayes classifier is a very efficient classifier as we can see that in the case of Training Iris data sets,  **only 20 samples were taken** and still the classifier could classify with a almost 95% precision. "
      ]
    }
  ]
}